{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
    "y_train = pd.read_csv('../data/processed/y_train.csv')\n",
    "X_valid = pd.read_csv('../data/processed/X_valid.csv')\n",
    "y_valid = pd.read_csv('../data/processed/y_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear un subconjunto balanceado 50-50\n",
    "def create_balanced_subset(X, y, sample_size=1000):\n",
    "    # Asumimos que 0 es la clase mayoritaria y 1 la minoritaria\n",
    "    X_majority = X[y.values.ravel() == 0]\n",
    "    X_minority = X[y.values.ravel() == 1]\n",
    "    y_majority = y[y.values.ravel() == 0]\n",
    "    y_minority = y[y.values.ravel() == 1]\n",
    "    \n",
    "    # Calcular el tamaño de cada clase en el nuevo subconjunto\n",
    "    n_each_class = min(len(X_minority), sample_size // 2)\n",
    "    \n",
    "    # Submuestrear la clase mayoritaria y sobremuestrear la minoritaria si es necesario\n",
    "    X_majority_downsampled = resample(X_majority, n_samples=n_each_class, random_state=42)\n",
    "    y_majority_downsampled = resample(y_majority, n_samples=n_each_class, random_state=42)\n",
    "    \n",
    "    if len(X_minority) < n_each_class:\n",
    "        X_minority_upsampled = resample(X_minority, n_samples=n_each_class, replace=True, random_state=42)\n",
    "        y_minority_upsampled = resample(y_minority, n_samples=n_each_class, replace=True, random_state=42)\n",
    "    else:\n",
    "        X_minority_upsampled = resample(X_minority, n_samples=n_each_class, replace=False, random_state=42)\n",
    "        y_minority_upsampled = resample(y_minority, n_samples=n_each_class, replace=False, random_state=42)\n",
    "    \n",
    "    # Combinar los subconjuntos balanceados\n",
    "    X_balanced = pd.concat([X_majority_downsampled, X_minority_upsampled])\n",
    "    y_balanced = pd.concat([y_majority_downsampled, y_minority_upsampled])\n",
    "    \n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop colums\n",
    "columns_to_drop = ['scaled__scaler__nr_employed', 'scaled__scaler__emp_var_rate']\n",
    "X_train = X_train.drop(columns=columns_to_drop)\n",
    "X_valid = X_valid.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un subconjunto balanceado\n",
    "X_train_balanced, y_train_balanced = create_balanced_subset(X_train, y_train, sample_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar el modelo\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    f1 = f1_score(y, y_pred)\n",
    "    f1_weighted = f1_score(y, y_pred, average='weighted')\n",
    "    auc_roc = roc_auc_score(y, y_pred_proba)\n",
    "    \n",
    "    return {\n",
    "        'F1-score': f1,\n",
    "        'F1-score weighted': f1_weighted,\n",
    "        'AUC-ROC': auc_roc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar los resultados\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluando Logistic Regression...\n",
      "Evaluando SVM...\n",
      "Evaluando Random Forest...\n",
      "Evaluando XGBoost...\n",
      "Evaluando LightGBM...\n",
      "[LightGBM] [Info] Number of positive: 2784, number of negative: 2784\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 643\n",
      "[LightGBM] [Info] Number of data points in the train set: 5568, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2227, number of negative: 2227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2227, number of negative: 2227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 641\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2227, number of negative: 2227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 636\n",
      "[LightGBM] [Info] Number of data points in the train set: 4454, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2228, number of negative: 2227\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500112 -> initscore=0.000449\n",
      "[LightGBM] [Info] Start training from score 0.000449\n",
      "[LightGBM] [Info] Number of positive: 2227, number of negative: 2228\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000404 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 638\n",
      "[LightGBM] [Info] Number of data points in the train set: 4455, number of used features: 36\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499888 -> initscore=-0.000449\n",
      "[LightGBM] [Info] Start training from score -0.000449\n"
     ]
    }
   ],
   "source": [
    "# Lista de modelos a explorar\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Explorar modelos\n",
    "for name, model in models.items():\n",
    "    print(f\"Evaluando {name}...\")\n",
    "    model.fit(X_train_balanced, y_train_balanced.values.ravel())\n",
    "    results[name] = evaluate_model(model, X_valid, y_valid)\n",
    "    \n",
    "    # Realizar validación cruzada\n",
    "    cv_scores = cross_val_score(model, X_train_balanced, y_train_balanced.values.ravel(), cv=5, scoring='f1')\n",
    "    results[name]['CV F1-score'] = cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados para Logistic Regression:\n",
      "F1-score: 0.5083\n",
      "F1-score weighted: 0.8606\n",
      "AUC-ROC: 0.8907\n",
      "CV F1-score: 0.8619\n",
      "\n",
      "Resultados para SVM:\n",
      "F1-score: 0.4279\n",
      "F1-score weighted: 0.7863\n",
      "AUC-ROC: 0.8820\n",
      "CV F1-score: 0.8877\n",
      "\n",
      "Resultados para Random Forest:\n",
      "F1-score: 0.4459\n",
      "F1-score weighted: 0.7829\n",
      "AUC-ROC: 0.9207\n",
      "CV F1-score: 0.8963\n",
      "\n",
      "Resultados para XGBoost:\n",
      "F1-score: 0.5045\n",
      "F1-score weighted: 0.8338\n",
      "AUC-ROC: 0.9211\n",
      "CV F1-score: 0.8932\n",
      "\n",
      "Resultados para LightGBM:\n",
      "F1-score: 0.5278\n",
      "F1-score weighted: 0.8456\n",
      "AUC-ROC: 0.9267\n",
      "CV F1-score: 0.8956\n"
     ]
    }
   ],
   "source": [
    "# Imprimir resultados\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\nResultados para {model}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Crear un DataFrame con los resultados para facilitar el ranking\n",
    "df_results = pd.DataFrame(results).T\n",
    "\n",
    "# Calcular un puntaje compuesto (promedio de las métricas)\n",
    "df_results['Composite Score'] = df_results[['F1-score', 'F1-score weighted', 'AUC-ROC', 'CV F1-score']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ranking de los 3 mejores algoritmos:\n",
      "                     F1-score  F1-score weighted   AUC-ROC  CV F1-score  \\\n",
      "LightGBM             0.527760           0.845583  0.926652     0.895630   \n",
      "XGBoost              0.504537           0.833846  0.921091     0.893238   \n",
      "Logistic Regression  0.508271           0.860617  0.890703     0.861869   \n",
      "\n",
      "                     Composite Score  \n",
      "LightGBM                    0.798907  \n",
      "XGBoost                     0.788178  \n",
      "Logistic Regression         0.780365  \n",
      "\n",
      "El mejor modelo basado en el puntaje compuesto es: LightGBM\n"
     ]
    }
   ],
   "source": [
    "# Ordenar los modelos basados en el puntaje compuesto\n",
    "ranked_models = df_results.sort_values('Composite Score', ascending=False)\n",
    "print(\"\\nRanking de los 3 mejores algoritmos:\")\n",
    "print(ranked_models[['F1-score', 'F1-score weighted', 'AUC-ROC', 'CV F1-score', 'Composite Score']].head(3))\n",
    "\n",
    "# Encontrar el mejor modelo basado en el puntaje compuesto\n",
    "best_model = ranked_models.index[0]\n",
    "print(f\"\\nEl mejor modelo basado en el puntaje compuesto es: {best_model}\")\n",
    "\n",
    "\n",
    "##### Agregar presicion como parametro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
